---
layout: post
title: 性能优化
categories: 程序设计
related_posts: True
tags: 优化设计
toc:
  sidebar: left
---

## 性能优化

### 1. concepts

#### 1.1 这些是一些常见操作的成本的数量级的概念:

时钟频率表示CPU每秒钟可以执行的周期数。时钟频率越高，CPU每秒钟可以执行的周期数就越多，处理速度也就越快。现代家用CPU的时钟频率通常在几GHz范围内。例如，3 GHz的CPU表示每秒钟有30亿个周期。

以3 GHz的CPU为例：
$ T = 1/f $
$ T= 1/(3\*10e9)=1/3 ns $

时间单位换算：
1 s = 10e3 ms = 10e6 μs = 10e8 ns

| 操作类型                                 | CPU时钟周期范围 | 时间范围（纳秒） |
| ---------------------------------------- | --------------- | ---------------- |
| 简单寄存器-寄存器操作（例如 ADD、OR 等） | <1              | <3ns             |
| 内存写入                                 | 1               | 3ns              |
| 绕过延迟：在整数和浮点单元之间切换       | 0-3             | 0-9ns            |
| 正确的 if 分支                           | 1-2             | 3-6ns            |
| char的比较                               | 1-3             | 3-6ns            |
| 浮点/向量加法                            | 1-3             | 3-9ns            |
| 乘法（整数/浮点/向量）                   | 1-7             | 3-21ns           |
| 返回错误和检查                           | 1-7             | 3-21ns           |
| L1 缓存读取                              | 3-4             | 9-12ns           |
| TLB 未命中                               | 7-21            | 21-63ns          |
| 上锁（互斥锁，非竞争）                   | 8 - 35          | 25ns - 100ns     |
| L2 缓存读取                              | 10-12           | 30-36ns          |
| 错误的 if 分支（分支错误预测）           | 10-20           | 30-60ns          |
| 浮点除法                                 | 10-40           | 30-120ns         |
| 128 位向量除法                           | 10-70           | 30-210ns         |
| 原子操作/CAS                             | 15-30           | 45-90ns          |
| C 函数直接调用                           | 15-30           | 45-90ns          |
| 整数除法                                 | 15-40           | 45-120ns         |
| C 函数间接调用                           | 20-50           | 60-150ns         |
| C++ 虚函数调用                           | 30-60           | 90-180ns         |
| L3 缓存读取                              | 30-70           | 90-210ns         |
| 主内存读取                               | 100-150         | 300-450ns        |
| NUMA（不同插槽的原子操作/CAS，估计值）   | 100-300         | 300-900ns        |
| NUMA（不同插槽的 L3 缓存读取）           | 100-300         | 300-900ns        |
| 分配+释放配对（小对象，内存池方式）      | 5-15            | 10-50ns          |
| 分配+释放配对（小对象，new和delete方式） | 200-500         | 600-1500ns       |
| NUMA（不同插槽的主内存读取）             | 300-500         | 900-1500ns       |
| -------------------------                | --------        | ----------       |
| 内核调用                                 | 1000-1500       | 3-4.5μs          |
| 线程上下文切换（直接成本）               | 2k              | 6μs              |
| C++ 异常抛出并捕获                       | 5k-50k          | 15-150μs         |
| 线程上下文切换（总成本，包括缓存失效）   | 10k- 1m         | 30μs - 3ms       |
| -------------------------                | --------        | ----------       |
| 磁盘访问（SSD）                          | 100k- 1m        | 30μs - 300μs     |
| 磁盘访问（HDD）                          | 1m- 10m         | 300μs - 3ms      |
| 线程创建                                 | 1m- 10m         | 300μs - 3ms      |
| 线程销毁                                 | 1m- 10m         | 300μs - 3ms      |
| 网络消息（本地网络）                     | 100k- 1m        | 30μs - 300μs     |
| 网络消息（广域网）                       | 1m- 1000m       | 300μs - 300ms    |

> - **L1缓存**：一般L1指令缓存的大小在32KB到128KB之间，L1数据缓存的大小也在32KB到128KB左右。
> - **L2缓存**：大小一般在256KB到2MB之间。
> - **L3缓存**：一般在4MB到32MB之间。

- **汇编指令**：汇编指令是人类可读的指令，通常直接对应于机器指令（CPU 指令）。每条汇编指令通常对应一条或多条机器指令。
- **指令执行过程**

  1. **取值（Fetch）**：从内存中取出指令。
  2. **译码（Decode）**：将取出的指令译码为 CP U 可以理解的操作。
  3. **执行（Execute）**：执行指令的操作，例如算术运算、逻辑运算、内存访问等。
  4. **写回（Write Back）**：将执行结果写回寄存器或内存。

- **时钟周期消耗**

  - **取值、译码、写回**：这些阶段的时钟周期消耗相对固定，通常都是一个时钟周期。
  - **执行**：执行阶段的时钟周期消耗取决于指令的类型，可以是一个时钟周期到几十个时钟周期。例如，简单的算术运算可能只需要一个时钟周期，而复杂的指令（如除法、内存访问）可能需要多个时钟周期。

- **超标量结构**
  - **超标量结构**：现代 CPU 通常使用超标量结构，可以在一个核内一个时钟周期内执行多条指令。这种结构通过增加多个执行单元，使得 CPU 可以并行处理多条指令，从而提高指令执行的效率。

#### 1.2 优化原则 && 代码优化主要思路

1. **二八原则**：
   - 80% 的性能问题通常来自 20% 的代码。
2. **先正确运行，再优化**：
   - 确保程序正确运行，然后再进行性能优化。
3. **不要太提前优化**：
   - 避免过早优化，先确保代码的正确性和可维护性。
4. **优化需要数据支持**：
   - 进行性能优化时需要有数据支持，编译器的优化能力很强，看上去有效的优化不一定实际有效。

- **性能优化原则**

  1. **二八原则**：

  - 80% 的性能问题通常来自 20% 的代码。

  2. **先正确运行，再优化**：

  - 确保程序正确运行，然后再进行性能优化。

  3. **不要太提前优化**：

  - 避免过早优化，先确保代码的正确性和可维护性。

  4. **优化需要数据支持**：

  - 进行性能优化时需要有数据支持，编译器的优化能力很强，看上去有效的优化不一定实际有效。

- **CPU 密集任务**

  1. **多线程**：

  - 使用多线程并行化计算任务，充分利用多核处理器的计算能力。
  - 使用 `wait` 机制协调线程间的同步。

  2. **优化算法实现**：

  - 避免不必要的递归，使用迭代替代递归。
  - 避免不必要的参数复制，使用引用传递参数。
  - 避免不必要的计算，减少重复计算。
  - 正确使用高效的算法和数据结构。

- **I/O 密集任务**

  1. **内存 I/O 密集**：

  - 使用对象池，避免重复构造和销毁对象。
  - 优化内存访问模式，使用顺序访问提高缓存命中率。

  2. **硬盘 I/O 密集**：

  - 使用异步 I/O 操作，允许程序在等待 I/O 操作完成时继续执行其他任务。
  - 使用缓存减少磁盘 I/O 操作，提高效率。
  - 使用高效的文件系统和磁盘调度算法。

  3. **网络 I/O 密集**：

  - 使用异步网络 I/O 操作，提高并发性和效率。
  - 使用高效的网络协议，减少网络延迟。
  - 使用压缩减少传输的数据量，提高传输效率。

- 优化方法在没有数据支持的情况下是不可靠的。
  也许你知道的的优化知识并没有跟上计算机的发展，换句话说，你知道的优化方法可能已经被编译器或者计算机底层硬件已经默认支持了。
  这个时候不急于数据支持，就基于优化理论进行改造，效果可能还不会有提升，甚至反向提升。

#### 1.3 代码实现常见原则

- **硬件友好代码**
  1. **利用 SIMD 特性**：
  - 现在的 CPU 很多时候都可以对指令并行化，利用 SIMD（Single Instruction, Multiple Data）特性进行向量化计算，提高并行计算性能。
  2. **提升缓存利用率**：
  - 使用结构化数组（SoA）和数组结构（AoS）方式优化数据布局，提高缓存命中率。
  - 避免缓存未命中，优化内存访问模式，使用顺序访问提高缓存命中率。

#### 1.4 性能优化常见手段

- 对象池
- 线程池
- 内存池
- 缓存

### 2. 实现优化

#### 2.1 RVO

- **什么是 RVO？RVO 有什么好处？**
  RVO，全称 Return Value Optimization（返回值优化），是 C++编译器的一种优化技术。当函数返回一个局部对象时，RVO 允许编译器省略额外的拷贝或移动构造函数的调用，直接在调用位置构造返回对象。

  RVO 的主要好处是提高性能。通过省略额外的拷贝或移动构造函数的调用，RVO 可以减少不必要的临时对象的创建，从而减少内存使用和提高运行速度。对于大型对象或者拷贝操作开销大的对象，RVO 的性能优势更为明显。

#### 2.2 soa 和 aos

- **soa和aos**
  让我们通过一个具体的例子来解释 SOA（Structure of Arrays）和 AOS（Array of Structures）。
  假设我们有一个粒子系统，每个粒子都有位置（x，y，z）和速度（vx，vy，vz）。

  ```cpp
  namespace AOS {
      struct Particle
      {
          float x, y, z;
          float vx, vy, vz;
      };
      std::array<Particle, 1000> particles;
      // 如果我们想更新所有粒子的位置，我们需要遍历整个数组
      // 这可能会导致缓存未命中，因为位置和速度数据在内存中是交错的。
      update(particles);
  }

  namespace SOA {
      struct Particles
      {
          std::array<float, 1000> x, y, z;
          std::array<float, 1000> vx, vy, vz;
      };

      Particles particles;
      // 如果我们想更新所有粒子的位置，我们可以连续地访问 x、y 和 z 数组，这有助于提高缓存命中率
      // 因为这些数据在内存中是连续的。
      update(particles);
  }
  ```

#### 2.3 CRTP

CRTP，全称 Curiously Recurring Template Pattern（奇异递归模板模式），是一种在 C++ 中使用的编程技巧。这种模式涉及到一个类模板，它以自己的派生类作为模板参数。**CRTP 可以用于实现编译时的多态性**，也就是说，**它可以在编译时决定调用哪个函数，而不是在运行时**。这可以提高性能，因为它**避免了虚函数调用的开销**。

    ```cpp
    template <typename Derived>
    class Base {
    public:
        void interface() {
            static_cast<Derived*>(this)->implementation();
        }
    };

    class Derived : public Base<Derived> {
    public:
        void implementation() {
            // 实现具体的功能
        }
    };
    ```

在这个例子中，`Base` 是一个模板类，它有一个 `interface` 方法，这个方法调用 `implementation` 方法。`Derived` 类继承自 `Base`，并提供 `implementation` 方法的实现。

当我们调用 `interface` 方法时，实际上调用的是 `Derived` 类的 `implementation` 方法。这是在编译时决定的，所以没有运行时的虚函数调用开销。

#### 2.4 CAS（Compare-and-Swap）

CAS，全称 Compare-and-Swap（比较并交换），是一种用于实现并发算法的原子操作。这种操作可以在多线程环境中安全地读取和更新共享数据，而无需使用锁。

CAS 操作接受三个参数：一个内存位置、一个预期的旧值和一个新值。如果内存位置的当前值与预期的旧值匹配，那么 CAS 操作就会将新值写入内存位置。否则，操作不会进行任何更改。无论哪种情况，CAS 操作都会返回内存位置的原始值。CAS 操作的一个关键特性是它是原子的，也就是说，它不会被其他线程的操作中断。这使得它在实现无锁数据结构和其他并发算法时非常有用。

    ```cpp
    int compare_and_swap(int* ptr, int old_val, int new_val) {
        int original_val = *ptr;
        if (original_val == old_val) {
            *ptr = new_val;
        }
        return original_val;
    }
    ```

在实际使用中，CAS 操作通常由硬件直接支持，并通过特殊的机器指令实现。在 C++ 中，你可以使用 `<atomic>` 头文件中的 `std::atomic` 类来进行 CAS 操作。

#### 2.5 COW（Copy On Write）

- **什么是 COW（Copy On Write）？有什么用？**
  COW，即 Copy On Write，是一种优化策略。在这种策略下，当对象被复制时，并不立即进行实际的复制操作，而是等到对象被修改时才进行复制。这种策略可以避免不必要的复制操作，从而提高程序的性能。

- **在什么场景下使用？**

  1.  **需要复制大量数据但不经常修改的场景**：

      - COW 技术通常在需要复制大量数据，但又不经常修改数据的场景中使用。例如，在字符串操作中，如果一个字符串被复制多次，但只有少数几次会修改字符串，那么使用 COW 技术可以大大提高性能。

  2.  **共享资源的场景**：
      - 在多线程或多进程环境中，多个线程或进程可以共享同一个资源，直到其中一个需要修改资源时才进行复制。这可以显著减少内存使用和提高性能。

- **怎么实现？**
  COW 技术的实现通常依赖于引用计数。以下是实现 COW 的步骤：
  1.  **引用计数**：
      - 每个对象都有一个引用计数，表示有多少个引用指向这个对象。当对象被复制时，不复制对象本身，而是增加引用计数。
  2.  **延迟复制**：
      - 当对象被修改时，首先检查引用计数。如果引用计数大于 1，那么就复制对象，然后修改复制后的对象，同时减少原对象的引用计数。如果引用计数等于 1，那么直接修改对象。

COW（Copy On Write）是一种优化策略，通过延迟复制操作直到对象被修改时才进行实际的复制，从而避免不必要的复制操作，提高程序性能。COW 技术通常在需要复制大量数据但不经常修改的场景中使用，其实现通常依赖于引用计数。通过合理使用 COW，可以显著减少内存使用和提高性能。

#### 2.6 考虑使用延迟计算

- **什么是延迟计算？**
  一个延迟计算的例子：

  class String{....}
  String s1 = "Hello";
  String s2 = s1; //在正常的情况下，这一句需要调用new操作符分配堆内存，然后调用strcpy将s1内的数据拷贝到s2里面。但是我们此时s2并没有被使用，所以我们不需要s2，这个时候如果让s2和s1共享一个值，就可以减小这些开销

  使用延迟计算进行读操作和写操作：

  String s = "Homer's Iliad";
  cout << s[3];
  s[3] = 'x';

  首先调用 operator[] 用来读取 string 的部分值，但是第二次调用该函数式为了完成写操作。读取效率较高，写入因为需要拷贝，所以效率较低，这个时候可以推迟作出是读操作还是写操作的决定。

  延迟策略进行数据库操作：有点类似之前写 web 的时候，把数据放在内存和数据库两份，更新的时候只更新内存，然后隔一段时间（或者等到使用的时候）去更新数据库。
  在 effective c++里面，则是更加专业的将这个操作封装成了一个类，然后把是否更新数据库弄成一个 flag。以及使用了 mutable 关键字，来修改数据

  延迟表达式：

  Matrix<int> m1(1000, 1000), m2(1000, 1000);
  m3 = m1 + m2;
  因为矩阵的加法计算量太大（1000\*1000）次计算，所以可以先用表达式表示m3是m1和m2的和，然后真正需要计算出值的时候再真的进行计算（甚至计算的时候也只计算m3[3][2]这样某一个位置的值）

### 3. 缓存

- 减少内存访问延迟：cpu缓存
- 减少磁盘IO读写延迟：文件缓存
- 减少数据库查询延迟：数据库缓存
- 减少网络IO读写延迟：web缓存/ 代理服务器缓存/ CDN缓存
- 减少cpu计算延迟：cacheData

#### 3.1 如何解决缓存空间受限问题

缓存为了快速访问一般都是通过哈希表的方式存储。
但当需要限制缓存空间大小的时候，即数据会不断增加，但缓存空间有限，就需要考虑那些缓存的数据需要被淘汰。
常见的淘汰方法有这些。

- LRU（Least Recently Used）：移除最近最少使用的数据。
- LFU（Least Frequently Used）：移除使用频率最低的数据。
- FIFO（First In First Out）：移除最早进入缓存的数据。

#### 3.2 如何提升cpu缓存命中率？

#### 3.3 如何提升内存缓存命中率？

### 4. 编译器优化

#### 4.1 优化级别

#### 4.2 pgo

PGO,全称为 Profile-Guided Optimization(基于分析的优化),是一种编译器优化技术,它通过收集程序运行时的性能信息来指导编译器优化.

PGO 的工作流程通常包括以下几个步骤:

1. **编译**:首先,你需要使用特殊的编译器选项来编译你的程序,这会使得程序在运行时收集性能信息.
2. **运行**:然后,你需要运行你的程序,并让它处理一些典型的工作负载.这会生成一个性能分析文件,其中包含了程序运行时的性能信息.
3. **优化编译**:最后,你需要再次使用特殊的编译器选项来编译你的程序,这次编译器会使用性能分析文件中的信息来指导优化.

通过这种方式,编译器可以了解到程序的实际运行情况,例如哪些代码经常被执行,哪些代码很少被执行,哪些条件分支的预测结果通常是什么等等.然后,编译器可以使用这些信息来进行更有效的优化,例如更精确地进行内联,更智能地安排代码布局,更准确地预测条件分支等等.

PGO 可以显著提高程序的运行速度,特别是对于那些性能敏感的应用程序.然而,PGO 也有一些缺点,例如它会增加编译时间和编译复杂性,因此并不是所有的项目都适合使用 PGO.

#### 4.3 lto

#### 4.4 尾递归优化

### 5. profiler

- gprof
- Valgrind
- perf
- Intel VTune Profiler
- vs profiler

### 98. attachment

#### 1. 流水线技术

这些时间范围是基于典型的硬件和操作系统环境，实际时间可能会因具体的硬件配置和系统负载而有所不同。

流水线技术，指的是一个指令没有完整执行完，另一个指令就已经在执行了是吧？

是的，你的理解是正确的。流水线技术的核心思想是将指令的执行过程分解成多个阶段，并允许这些阶段并行处理不同的指令。这样，当一个指令还在执行的过程中，另一个指令就可以开始执行它的下一个阶段。这种并行性可以显著提高处理器的效率和吞吐量。

具体来说，流水线技术的工作方式如下：

1. **取值（Fetch）**：从内存中取出指令。
2. **译码（Decode）**：将取出的指令译码为CPU可以理解的操作。
3. **执行（Execute）**：执行指令的操作，例如算术运算、逻辑运算、内存访问等。
4. **写回（Write Back）**：将执行结果写回寄存器或内存。

在流水线中，这四个阶段可以同时进行，每个阶段处理不同的指令。例如：

- 指令1在执行阶段。
- 指令2在写回阶段。
- 指令3在译码阶段。
- 指令4在取值阶段。

这种并行处理可以减少每个指令完成所需的总时间，因为每个时钟周期内都有一个指令的某个阶段被处理。然而，流水线技术也引入了一些挑战，如流水线冒险（数据冒险、控制冒险和结构冒险），需要通过各种技术（如转发、暂停流水线、分支预测等）来管理和解决。

总之，流水线技术通过允许指令在不同阶段并行执行，提高了处理器的效率和吞吐量，但同时也需要处理由此引入的复杂性和挑战。

如果这些都存在的话，执行一个寄存器加法命令，最少需要4个时钟周期，是嘛？

执行一个寄存器加法命令确实会经历取值、译码、执行和写回这四个阶段，但是否需要四个时钟周期取决于具体的处理器设计和流水线的实现方式。

在没有流水线的简单处理器中，每个指令确实需要四个时钟周期来完成，因为每个阶段必须在前一个阶段完成后才能开始。这意味着如果一个寄存器加法指令正在执行，它将独占处理器四个周期。

然而，在流水线处理器中，情况就不同了。流水线处理器将这四个阶段分开，并允许每个阶段同时处理不同的指令。例如，当一个寄存器加法指令在执行阶段时，另一个指令可以同时在取值阶段，第三个指令在译码阶段，以此类推。这样，尽管每个指令仍然需要经过这四个阶段，但流水线的设计允许在每个时钟周期内完成一个指令的某个阶段，从而减少了每个指令的平均执行时间。

在流水线处理器中，一个寄存器加法指令可能只需要一个时钟周期就可以完成，但这取决于流水线的宽度和是否有任何流水线冒险（如数据冒险、控制冒险等）需要处理。如果流水线是无冒险的，并且有足够的执行单元来处理指令，那么每个指令可以在每个时钟周期内完成一个阶段，从而在四个时钟周期内完成整个指令。

总结来说，虽然指令执行的基本阶段（取值、译码、执行、写回）是固定的，但是否需要四个时钟周期来完成一个寄存器加法指令取决于处理器的具体设计和是否使用了流水线技术。

### 99. quiz

#### 1. 服务器高并发的解决方案

1. 应用数据与静态资源分离
   将静态资源(图片,视频,js,css等)单独保存到专门的静态资源服务器中,在客户端访问的时候从静态资源服务器中返回静态资源,从主服务器中返回应用数据.

2. 客户端缓存
   因为效率最高,消耗资源最小的就是纯静态的html页面,所以可以把网站上的页面尽可能用静态的来实现,在页面过期或者有数据更新之后再将页面重新缓存.或者先生成静态页面,然后用ajax异步请求获取动态数据.

3. 集群和分布式
   (集群是所有的服务器都有相同的功能,请求哪台都可以,主要起分流作用)<br>
   (分布式是将不同的业务放到不同的服务器中,处理一个请求可能需要使用到多台服务器,起到加快请求处理的速度.)<br>
   可以使用服务器集群和分布式架构,使得原本属于一个服务器的计算压力分散到多个服务器上.同时加快请求处理的速度.

4. 反向代理
   在访问服务器的时候,服务器通过别的服务器获取资源或结果返回给客户端.

- 假设在某一时刻由几万个并发请求同时产生,请设计一个方案来处理这种情况.

#### 2. 虚函数表开销

虚函数表（vtable）的使用确实会引入一些性能开销，主要体现在以下几个方面：

- **1. 虚函数调用的间接寻址**

  - **解释**：虚函数调用需要通过虚函数表进行间接寻址，这比直接调用函数要慢。
  - **原因**：编译器在编译时无法确定具体调用哪个函数，因此在运行时需要通过虚函数表查找函数地址，然后进行调用。这种间接寻址会增加指令开销。

- **2. 破坏指令缓存（Instruction Cache）**

  - **解释**：虚函数调用可能会破坏指令缓存（Instruction Cache），因为虚函数表中的函数地址可能指向不同的内存位置。
  - **原因**：现代处理器使用指令缓存来加速指令的执行。当程序执行过程中频繁跳转到不同的内存位置时，指令缓存可能会失效（cache miss），导致性能下降。

- **3. 破坏数据缓存（Data Cache）**
  - **解释**：虚函数表的访问可能会破坏数据缓存（Data Cache），因为虚函数表和实际函数实现可能位于不同的内存区域。
  - **原因**：数据缓存用于加速数据的访问。当程序访问虚函数表时，可能会导致数据缓存失效（cache miss），从而增加内存访问延迟。

#### 3. 什么时候该使用对象池？

对于平凡构造的对象来说，使用对象池没有多大意义。
因为如果复用对象的数据需要修改，其实和重新构造一个差不多了。如一般的`Point`对象，使用池化，就是多此一举的。
因此对象池一般是针对构造函数实现有特殊处理的，大多为涉及资源，而非数据的。比如说`socketFd`，这是一个系统资源，要走到内核态去申请和释放，这个使用池化技术是有意义的。
